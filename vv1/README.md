웹서버를 aws 기반에서 azure 기반으로 교체하였다.
사용하는 SQL도 Azure SQL Database로 바꿨다. 역시 SQL 적응에 오랜 시간이 걸렸다..

우선 고티어 계정의 대전기록을 통해 gameid 리스트를 받아오던 기존 방식에서,
모든 gameid를 검색하여 그 중 한국 서버 솔로 랭크 게임만 선택하는 방식으로 변경하였다.
상위 티어 제한을 없애니 각 패치 버전마다 500만개 이상의 데이터를 확보할 수 있었다.

그리고 API 사용 제한을 많이 잡아먹던 평균 티어 정보를 제외하였다.
티어에 따라 별도로 분석하는 것은 불가능해졌지만, 빠른 시간에 많은 정보를 받아올 수 있게 되었다.
시즌 초라 티어 정보가 부정확할 수 있다는 점도 있었다.

10시즌 분석(깃헙에는 v버전)에서 event나 team summary 같은 테이블은 분석에 사용하지 않는다는 것을 확인하고,
DB 용량과 저장 시간만 잡아먹는 얘들을 과감하게 삭제했다.

포지션마다 테이블을 분리해서 각 게임이 테이블에서 두 개의 row만 차지하도록 하였다.
팀까지 분리해서 b_top, r_top ... 이렇게 할까 하다가 같은 포지션끼리 통계를 합칠 때 시간이 오래 걸릴 것 같아 포지션으로만 분리하였다.
한 개의 게임이 각 테이블에 두 개의 row로만 들어가기 때문에, gameid를 SQL join을 위한 key로 사용할 수 있을 것이라 생각하였다.
gameid가 10^9 단위로 SQL integer의 범위를 아슬아슬하게 넘어 bigint로 저장해야했다.
그러나 이는 낭비라고 생각하여 시즌 11의 gameid 최솟값보다 살짝 작은 값을 기준값으로 설정해
(gameid - 기준값) * 2 + (0 if team==blue else 1) 값을 테이블의 key로 설정하였다.
따라서 key를 통해 gameid를 역연산하여 찾을수도 있다.

모든 게임을 확인한 뒤, 모든 구간에 비슷한 수의 게임이 들어갈 수 있도록 게임 길이에 따라 10개의 구간으로 나눴다.
데이터 지표는 게임 길이에 따라 크기의 가치가 달라지기 때문에, 게임 길이에 의한 효과를 최소한으로 두기 위함이다.
각 구간에서 별도로 평균과 표준편차를 구해 데이터로 사용하였다.
처음에는 이 연산을 SQL로 진행하였는데, Python Numpy로 진행하는 것이 더 빠르다는 것을 확인한 후엔 Python을 이용하여 진행하였다.
다만 Python에서 진행하기 위해서는 모든 데이터를 받아와야하기때문에 인터넷 속도나 환경에 따라 문제가 생길 수 있고,
표준화한 데이터를 다시 저장하는 것에도 시간이 필요하다는 차이가 있다.
